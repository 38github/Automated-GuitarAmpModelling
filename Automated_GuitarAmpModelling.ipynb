{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inspect GPU configuration\n",
    "# to inspect full configuration run nvidia-smi on the host\n",
    "# running this container\n",
    "import torch"
    "if not torch.cuda.is_available():\n",
    "    print('Not connected to a GPU')\n",
    "else:\n",
    "    print(\"Detected %s gpus\" % torch.cuda.device_count())\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "# Inspect software configuration\n",
    "pytorchv = !pip3 show torch | grep Version\n",
    "print(\"Pytorch version: %s\" % pytorchv)\n",
    "tensorflowv = !pip3 show tensorflow | grep Version\n",
    "print(\"Tensorflow version: %s\" % tensorflowv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Git repo clone\n",
    "#!git clone https://github.com/MaxPayne86/Automated-GuitarAmpModelling.git\n",
    "#!cd Automated-GuitarAmpModelling && git checkout feature/add-custom-bounds && git submodule update --init --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "class StopExecution(Exception):\n",
    "  def _render_traceback_(self):\n",
    "    pass\n",
    "\n",
    "def parse_stats(stats):\n",
    "  with open(stats) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    test_lossESR_final = data['test_lossESR_final']\n",
    "    test_lossESR_best = data['test_lossESR_best']\n",
    "    esr = min(test_lossESR_final, test_lossESR_best)\n",
    "    valuedict = {}\n",
    "    valuedict[\"model_name\"] = model_name\n",
    "    valuedict[\"unit_type\"] = unit_type\n",
    "    valuedict[\"hidden_size\"] = hidden_size\n",
    "    valuedict[\"skip\"] = skip\n",
    "    return [esr, valuedict]\n",
    "\n",
    "def write_results_dict(resultsdict, outfile):\n",
    "  orderedlist = sorted(resultsdict)\n",
    "  counter = 1\n",
    "  for item in orderedlist:\n",
    "    print(\"%d) %s: %s %d | skip = %d | ESR = %.6f\" % (counter, resultsdict[item]['model_name'], resultsdict[item]['unit_type'], resultsdict[item]['hidden_size'], resultsdict[item]['skip'], item))\n",
    "    counter = counter + 1\n",
    "  with open(outfile, 'w') as fp:\n",
    "    counter = 1\n",
    "    for item in orderedlist:\n",
    "      line = \"%d) %s: %s %d | skip = %d | ESR = %.6f\" % (counter, resultsdict[item]['model_name'], resultsdict[item]['unit_type'], resultsdict[item]['hidden_size'], resultsdict[item]['skip'], item)\n",
    "      fp.write(\"%s\\n\" % line)\n",
    "      counter = counter + 1\n",
    "\n",
    "# Do not use tensorflow for training in this container (installation is broken)\n",
    "# it is just there for the modelToKeras.py script to generate an RTNeural compatible model file\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Logs all tensorflow messages except INFO, WARNING, and ERROR\n",
    "\n",
    "# Pre-defined network types list\n",
    "types_list_full = [\"GRU-8\", \"GRU-12\", \"LSTM-8\", \"LSTM-12\", \"LSTM-16\", \"LSTM-20\"]\n",
    "types_list_heavy = [\"LSTM-16\", \"LSTM-20\", \"LSTM-40\"]\n",
    "types_list_light = [\"GRU-12\", \"LSTM-12\", \"LSTM-16\"]\n",
    "\n",
    "# In case of a conditioned (parameterized) model, the input\n",
    "# and output tracks are provided in a params dict\n",
    "params = {\n",
    "    \"n\": 1,\n",
    "    \"datasets\": [\n",
    "    {\n",
    "        \"params\":[ 0.0 ],\n",
    "        \"input\": \"Recordings/cdricliz/in-gtr-x-org.wav\",\n",
    "        \"target\": \"Recordings/cdricliz/FENDER_TWIN_VERB_65_2.wav\"\n",
    "    },\n",
    "    {\n",
    "        \"params\":[ 0.25 ],\n",
    "        \"input\": \"Recordings/cdricliz/in-gtr-x-org.wav\",\n",
    "        \"target\": \"Recordings/cdricliz/FENDER_TWIN_VERB_65_4.wav\"\n",
    "    },\n",
    "    {\n",
    "        \"params\":[ 0.5 ],\n",
    "        \"input\": \"Recordings/cdricliz/in-gtr-x-org.wav\",\n",
    "        \"target\": \"Recordings/cdricliz/FENDER_TWIN_VERB_65_6.wav\"\n",
    "    },\n",
    "    {\n",
    "        \"params\":[ 0.75 ],\n",
    "        \"input\": \"Recordings/cdricliz/in-gtr-x-org.wav\",\n",
    "        \"target\": \"Recordings/cdricliz/FENDER_TWIN_VERB_65_8.wav\"\n",
    "    },\n",
    "    {\n",
    "        \"params\":[ 1.0 ],\n",
    "        \"input\": \"Recordings/cdricliz/in-gtr-x-org.wav\",\n",
    "        \"target\": \"Recordings/cdricliz/FENDER_TWIN_VERB_65_10.wav\"\n",
    "    }]\n",
    "}\n",
    "\n",
    "# You can also specify metadata to add into the json file\n",
    "metadata = {\n",
    "    \"name\": \"fnd-twin-rev-aidadsp\",\n",
    "    \"samplerate\": \"48000\",\n",
    "    \"source\":\"original\",\n",
    "    \"style\": \"clean / breakup\",\n",
    "    \"based\": \"Fender Twin Reverb 2014\",  \n",
    "    \"author\": \"Aida DSP\",\n",
    "    \"dataset\": \"custom\",\n",
    "    \"license\": \"CC BY-NC-ND 4.0\" \n",
    "}\n",
    "\n",
    "# A model is defined by name and input/output track(s) plus network type and config\n",
    "model_list = [\n",
    "{\n",
    "    \"metadata\": metadata,\n",
    "    \"params\": params,\n",
    "    \"blip_locations\": [18_770, 66_770],\n",
    "    \"blip_window\": 78_770,\n",
    "    \"csv\": \"yes\",\n",
    "    \"types_list\": [\"LSTM-12\"],\n",
    "    \"skip\": [1]\n",
    "}]\n",
    "\n",
    "# Print model_list \n",
    "#for model in model_list:\n",
    "#  print(\"%s\\n %s\\n %s\\n %s\" % (model['metadata']['name'], model['input'], model['target'], model['csv']))\n",
    "#  for type in model['types_list']:\n",
    "#    print(\"  %s\" % type)\n",
    "#  for value in model['skip']:\n",
    "#    print(\"  %d\" % int(value))\n",
    "#raise StopExecution\n",
    "\n",
    "# Perform the training for each model\n",
    "import json\n",
    "!export CUBLAS_WORKSPACE_CONFIG=:4096:2\n",
    "for model in model_list:\n",
    "  resultsdict = {}\n",
    "  for skip in model['skip']:\n",
    "    for type in model['types_list']:\n",
    "      # Write config file under ./Configs on-the-fly\n",
    "      types_args = type.split('-')\n",
    "      unit_type = types_args[0]\n",
    "      hidden_size = int(types_args[1])\n",
    "      loss_functions = {\"ESR\": 0.75, \"DC\": 0.25}\n",
    "      pre_filt = \"A-Weighting\"\n",
    "      device = \"aidadsp-auto\"\n",
    "      file_name = \"aidadsp-auto\"\n",
    "      model_name = \"RNN-\" + device\n",
    "      config = \"Configs/RNN-%s.json\" % device\n",
    "      # Create config file on the fly\n",
    "      config_data = {}\n",
    "      config_data['hidden_size'] = hidden_size\n",
    "      config_data['unit_type'] = unit_type\n",
    "      config_data['loss_functions'] = loss_functions\n",
    "      config_data['pre_filt'] = pre_filt\n",
    "      config_data['skip_con'] = skip\n",
    "      config_data['device'] = device\n",
    "      config_data['file_name'] = file_name\n",
    "      config_data['metadata'] = model['metadata']\n",
    "      try:\n",
    "        config_data['params'] = model['params']\n",
    "      except KeyError:\n",
    "        model['params'] = None\n",
    "        config_data['params'] = model['params']\n",
    "      try:\n",
    "        config_data['blip_locations'] = model['blip_locations']\n",
    "      except KeyError:\n",
    "        model['blip_locations'] = None\n",
    "        config_data['blip_locations'] = model['blip_locations']\n",
    "      try:\n",
    "        config_data['blip_window'] = model['blip_window']\n",
    "      except KeyError:\n",
    "        model['blip_window'] = None\n",
    "        config_data['blip_window'] = model['blip_window']\n",
    "      json_string = json.dumps(config_data)\n",
    "      json_file = open(config, \"w\")\n",
    "      json_file.write(json_string)\n",
    "      json_file.close()\n",
    "      # Create dataset on-the-fly\n",
    "      if model['params']:\n",
    "        if model['csv'] == \"yes\":\n",
    "          !python3 prep_wav.py -l $model_name -p -csv\n",
    "        else:\n",
    "          !python3 prep_wav.py -l $model_name -p\n",
    "      else:\n",
    "        input_file = model['input']\n",
    "        target_file = model['target']\n",
    "        if model['csv'] == \"yes\":\n",
    "          !python3 prep_wav.py -f \"$input_file\" \"$target_file\" -l $model_name -csv\n",
    "        else:\n",
    "          !python3 prep_wav.py -f \"$input_file\" \"$target_file\" -l $model_name\n",
    "      # Perform training\n",
    "      from time import time, gmtime, strftime\n",
    "      start = time()\n",
    "      if model['params']:\n",
    "        input_size = model['params']['n'] + 1\n",
    "        !python3 dist_model_recnet.py -l $model_name -slen 24000 --seed 39 -lm 0 -is $input_size\n",
    "      else:\n",
    "        !python3 dist_model_recnet.py -l $model_name -slen 24000 --seed 39 -lm 0\n",
    "      end = time()\n",
    "      delta = end - start\n",
    "      print(strftime(\"%H:%M:%S\", gmtime(delta)))\n",
    "      # Convert json model file to RTNural format\n",
    "      !python3 modelToKeras.py -l $model_name\n",
    "      # Copy meaningful data to an output dir for later analysis\n",
    "      result_dir = \"Results/\" + device + '_' + unit_type + '-' + str(hidden_size) + '-' + str(skip)\n",
    "      now = !date +\"%Y-%m-%d-%H:%M:%S\"\n",
    "      output_dir = \"Results/Jupyter/%s_%s\" % (now[0], model['metadata']['name'].replace(' ', '_'))\n",
    "      !mkdir -p $output_dir\n",
    "      #!tar czf $output_dir/Data.tar.gz Data\n",
    "      !tar czf $output_dir/TensorboardData.tar.gz TensorboardData\n",
    "      !rm -rf TensorboardData\n",
    "      !cp $result_dir/model*.json $result_dir/training_stats.json $output_dir/\n",
    "      !cp $config $output_dir/\n",
    "      new_model_file_name = model['metadata']['name']+\".json\"\n",
    "      !cp $result_dir/model_keras.json \"$output_dir/$new_model_file_name\"\n",
    "      # Update dict with training results\n",
    "      stats = result_dir + \"/training_stats.json\"\n",
    "      [esr, entry] = parse_stats(stats)\n",
    "      resultsdict[esr] = entry\n",
    "      !rm -rf Results/*$model_name\n",
    "  # Write sorted results on a file\n",
    "  result_file = \"Results/Jupyter/%s_%s.txt\" % (now[0], model['name'].replace(' ', '_'))\n",
    "  write_results_dict(resultsdict, result_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Automated-GuitarAmpModelling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
